{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#About-this-Assignment\" data-toc-modified-id=\"About-this-Assignment-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>About this Assignment</a></span></li><li><span><a href=\"#Load-and-Prepare-Data\" data-toc-modified-id=\"Load-and-Prepare-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load and Prepare Data</a></span></li><li><span><a href=\"#Split-data-to-Train-and-Test-sets\" data-toc-modified-id=\"Split-data-to-Train-and-Test-sets-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Split data to Train and Test sets</a></span></li><li><span><a href=\"#Multinomial-Naive-Bayes\" data-toc-modified-id=\"Multinomial-Naive-Bayes-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Multinomial Naive Bayes</a></span></li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Logistic Regression</a></span></li><li><span><a href=\"#XGBoost\" data-toc-modified-id=\"XGBoost-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>XGBoost</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About this Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to be able to classify new \"test\" documents using already classified \"training\" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  Here is one example of such data:  [UCI Machine Learning Repository: Spambase Data Set](http://archive.ics.uci.edu/ml/datasets/Spambase)\n",
    "<br><br>\n",
    "For this project, we'll use the above dataset to predict the class of new documents withheld from the training dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary dataset is a collection of 58 columns... <br>\n",
    "\n",
    "<I> Columns 1 thru 48:</I> continuous real attributes of type word_freq_WORD.  Percentage of words in the e-mail that match word, i.e. 100 * (number of times the word appears in the e-mail) / total number of words in e-mail.\n",
    "\n",
    "<I>Columns 49 thru 54:</I> continuous real attributes of type char_freq_CHAR. Percentage of characters in the e-mail that match CHAR, i.e. 100 * (number of CHAR occurences) / total characters in e-mail\n",
    "\n",
    "<I>Column 55:</I> attribute of type capital_run_length_average = average length of uninterrupted sequences of capital letters\n",
    "\n",
    "<I>Column 56:</I> attribute of type capital_run_length_longest = length of longest uninterrupted sequence of capital letters\n",
    "\n",
    "<I>Column 57:</I> attribute of type capital_run_length_total = total number of capital letters in the e-mail\n",
    "\n",
    "<I>Column 58:</I> attribute of type spam -- denotes whether the e-mail was considered spam (1) or not (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Documents\\00_Applications_DataScience\\CUNY\\DATA620\\KJW_CUNY_DATA_620\\Week5_Part2 (Document Classification)\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\user\\Documents\\00_Applications_DataScience\\CUNY\\DATA620\\KJW_CUNY_DATA_620\\Week5_Part2 (Document Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Spambase dataset from UCI\n",
    "data = pd.read_csv('spambase.data', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct a header and apply it to the dataframe\n",
    "word_freq = ['word_freq' + str(r+1) for r in data.columns[0:48]]\n",
    "char_freq = ['char_freq' + str(r+1) for r in data.columns[48:54]]\n",
    "\n",
    "data_header = word_freq + char_freq + ['cap_letter_avg'] + ['cap_letter_longest'] + ['cap_letter_total'] + ['spam_indicator']\n",
    "\n",
    "data.columns = data_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq1</th>\n",
       "      <th>word_freq2</th>\n",
       "      <th>word_freq3</th>\n",
       "      <th>word_freq4</th>\n",
       "      <th>word_freq5</th>\n",
       "      <th>word_freq6</th>\n",
       "      <th>word_freq7</th>\n",
       "      <th>word_freq8</th>\n",
       "      <th>word_freq9</th>\n",
       "      <th>word_freq10</th>\n",
       "      <th>word_freq11</th>\n",
       "      <th>word_freq12</th>\n",
       "      <th>word_freq13</th>\n",
       "      <th>word_freq14</th>\n",
       "      <th>word_freq15</th>\n",
       "      <th>word_freq16</th>\n",
       "      <th>word_freq17</th>\n",
       "      <th>word_freq18</th>\n",
       "      <th>word_freq19</th>\n",
       "      <th>word_freq20</th>\n",
       "      <th>word_freq21</th>\n",
       "      <th>word_freq22</th>\n",
       "      <th>word_freq23</th>\n",
       "      <th>word_freq24</th>\n",
       "      <th>word_freq25</th>\n",
       "      <th>word_freq26</th>\n",
       "      <th>word_freq27</th>\n",
       "      <th>word_freq28</th>\n",
       "      <th>word_freq29</th>\n",
       "      <th>word_freq30</th>\n",
       "      <th>word_freq31</th>\n",
       "      <th>word_freq32</th>\n",
       "      <th>word_freq33</th>\n",
       "      <th>word_freq34</th>\n",
       "      <th>word_freq35</th>\n",
       "      <th>word_freq36</th>\n",
       "      <th>word_freq37</th>\n",
       "      <th>word_freq38</th>\n",
       "      <th>word_freq39</th>\n",
       "      <th>word_freq40</th>\n",
       "      <th>word_freq41</th>\n",
       "      <th>word_freq42</th>\n",
       "      <th>word_freq43</th>\n",
       "      <th>word_freq44</th>\n",
       "      <th>word_freq45</th>\n",
       "      <th>word_freq46</th>\n",
       "      <th>word_freq47</th>\n",
       "      <th>word_freq48</th>\n",
       "      <th>char_freq49</th>\n",
       "      <th>char_freq50</th>\n",
       "      <th>char_freq51</th>\n",
       "      <th>char_freq52</th>\n",
       "      <th>char_freq53</th>\n",
       "      <th>char_freq54</th>\n",
       "      <th>cap_letter_avg</th>\n",
       "      <th>cap_letter_longest</th>\n",
       "      <th>cap_letter_total</th>\n",
       "      <th>spam_indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.28</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq1  word_freq2  word_freq3  word_freq4  word_freq5  word_freq6  \\\n",
       "0        0.00        0.64        0.64         0.0        0.32        0.00   \n",
       "1        0.21        0.28        0.50         0.0        0.14        0.28   \n",
       "2        0.06        0.00        0.71         0.0        1.23        0.19   \n",
       "3        0.00        0.00        0.00         0.0        0.63        0.00   \n",
       "4        0.00        0.00        0.00         0.0        0.63        0.00   \n",
       "\n",
       "   word_freq7  word_freq8  word_freq9  word_freq10  word_freq11  word_freq12  \\\n",
       "0        0.00        0.00        0.00         0.00         0.00         0.64   \n",
       "1        0.21        0.07        0.00         0.94         0.21         0.79   \n",
       "2        0.19        0.12        0.64         0.25         0.38         0.45   \n",
       "3        0.31        0.63        0.31         0.63         0.31         0.31   \n",
       "4        0.31        0.63        0.31         0.63         0.31         0.31   \n",
       "\n",
       "   word_freq13  word_freq14  word_freq15  word_freq16  word_freq17  \\\n",
       "0         0.00         0.00         0.00         0.32         0.00   \n",
       "1         0.65         0.21         0.14         0.14         0.07   \n",
       "2         0.12         0.00         1.75         0.06         0.06   \n",
       "3         0.31         0.00         0.00         0.31         0.00   \n",
       "4         0.31         0.00         0.00         0.31         0.00   \n",
       "\n",
       "   word_freq18  word_freq19  word_freq20  word_freq21  word_freq22  \\\n",
       "0         1.29         1.93         0.00         0.96          0.0   \n",
       "1         0.28         3.47         0.00         1.59          0.0   \n",
       "2         1.03         1.36         0.32         0.51          0.0   \n",
       "3         0.00         3.18         0.00         0.31          0.0   \n",
       "4         0.00         3.18         0.00         0.31          0.0   \n",
       "\n",
       "   word_freq23  word_freq24  word_freq25  word_freq26  word_freq27  \\\n",
       "0         0.00         0.00          0.0          0.0          0.0   \n",
       "1         0.43         0.43          0.0          0.0          0.0   \n",
       "2         1.16         0.06          0.0          0.0          0.0   \n",
       "3         0.00         0.00          0.0          0.0          0.0   \n",
       "4         0.00         0.00          0.0          0.0          0.0   \n",
       "\n",
       "   word_freq28  word_freq29  word_freq30  word_freq31  word_freq32  \\\n",
       "0          0.0          0.0          0.0          0.0          0.0   \n",
       "1          0.0          0.0          0.0          0.0          0.0   \n",
       "2          0.0          0.0          0.0          0.0          0.0   \n",
       "3          0.0          0.0          0.0          0.0          0.0   \n",
       "4          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   word_freq33  word_freq34  word_freq35  word_freq36  word_freq37  \\\n",
       "0          0.0          0.0          0.0          0.0         0.00   \n",
       "1          0.0          0.0          0.0          0.0         0.07   \n",
       "2          0.0          0.0          0.0          0.0         0.00   \n",
       "3          0.0          0.0          0.0          0.0         0.00   \n",
       "4          0.0          0.0          0.0          0.0         0.00   \n",
       "\n",
       "   word_freq38  word_freq39  word_freq40  word_freq41  word_freq42  \\\n",
       "0          0.0          0.0         0.00          0.0          0.0   \n",
       "1          0.0          0.0         0.00          0.0          0.0   \n",
       "2          0.0          0.0         0.06          0.0          0.0   \n",
       "3          0.0          0.0         0.00          0.0          0.0   \n",
       "4          0.0          0.0         0.00          0.0          0.0   \n",
       "\n",
       "   word_freq43  word_freq44  word_freq45  word_freq46  word_freq47  \\\n",
       "0         0.00          0.0         0.00         0.00          0.0   \n",
       "1         0.00          0.0         0.00         0.00          0.0   \n",
       "2         0.12          0.0         0.06         0.06          0.0   \n",
       "3         0.00          0.0         0.00         0.00          0.0   \n",
       "4         0.00          0.0         0.00         0.00          0.0   \n",
       "\n",
       "   word_freq48  char_freq49  char_freq50  char_freq51  char_freq52  \\\n",
       "0          0.0         0.00        0.000          0.0        0.778   \n",
       "1          0.0         0.00        0.132          0.0        0.372   \n",
       "2          0.0         0.01        0.143          0.0        0.276   \n",
       "3          0.0         0.00        0.137          0.0        0.137   \n",
       "4          0.0         0.00        0.135          0.0        0.135   \n",
       "\n",
       "   char_freq53  char_freq54  cap_letter_avg  cap_letter_longest  \\\n",
       "0        0.000        0.000           3.756                  61   \n",
       "1        0.180        0.048           5.114                 101   \n",
       "2        0.184        0.010           9.821                 485   \n",
       "3        0.000        0.000           3.537                  40   \n",
       "4        0.000        0.000           3.537                  40   \n",
       "\n",
       "   cap_letter_total  spam_indicator  \n",
       "0               278               1  \n",
       "1              1028               1  \n",
       "2              2259               1  \n",
       "3               191               1  \n",
       "4               191               1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data to Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let x be all the predictor data and y be the spam indicator column that we are trying to predict\n",
    "X = data.iloc[:, 0:57]\n",
    "y = data.spam_indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the predicted tags\n",
    "pred = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7972350230414746\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score\n",
    "score = accuracy_score(y_test,pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[789 149]\n",
      " [159 422]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the confusion matrix: cm\n",
    "cm = confusion_matrix(y_test,pred,labels=[0,1])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This article from Towards Data Science titled, [\"Building a Logistic Regression Model in Python, Step by Step\"](https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8) was helpful for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "lr_classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Fit the classifier to the training data\n",
    "\n",
    "rfe = RFE(lr_classifier, 57)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the predicted tags\n",
    "pred = rfe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9374588545095458\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score\n",
    "rfe_score = accuracy_score(y_test,pred)\n",
    "print(rfe_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[901  37]\n",
      " [ 58 523]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the confusion matrix: cm\n",
    "xgb_cm = confusion_matrix(y_test,pred,labels=[0,1])\n",
    "print(xgb_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "xgb_classifier = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the classifier to the training data\n",
    "xgb_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the predicted tags\n",
    "pred = xgb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9578670177748518\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score\n",
    "xgb_score = accuracy_score(y_test,pred)\n",
    "print(xgb_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[917  21]\n",
      " [ 43 538]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the confusion matrix: cm\n",
    "xgb_cm = confusion_matrix(y_test,pred,labels=[0,1])\n",
    "print(xgb_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the three models (Naive Bayes, Logistic Regression, and XGBoost), both Logistic Regression and XGBoost performed well with accuracy scores of 93.7% and 95.7% respectively.  The confusion matrix's false positive (FP) and false negatives (FN) for each of these models were very close, with XGBoost having only 15 less FP and 16 less FN than Logistic Regression.\n",
    "\n",
    "Of interest was that running a feature selection for the Logistic Regression model yielded a result of all 57 columns being relevant to obtain the high accuracy and confusion matrix scores.  This led to a decision to not drop any of the data columns, which tends to be an unusual result.\n",
    "\n",
    "If this project was \"real life\", XGBoost would be the model that would be used to run against additional datasets and ultimately move forward into a production environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
